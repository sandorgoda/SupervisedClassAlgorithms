{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  E-Mail classification problem\n",
    "## We have a set of emails, half of which were written by one person and the other half by another person at the same company . Our objective is to classify the emails as written by one person or the other based only on the text of the email. We will start with Naive Bayes in this mini-project, and then expand in later projects to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Naive Bayes\n",
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import cPickle\n",
    "import numpy \n",
    "from time import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess(words_file = \"word_data.pkl\", \n",
    "               authors_file=\"email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = cPickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = sklearn.model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "15820\n",
      "15820\n",
      "training time: 1.268 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Use a Naive Bayes Classifier to identify emails by their authors\n",
    "    \n",
    "    authors and labels:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "print len(features_train)\n",
    "print len(labels_train)\n",
    "\n",
    "#Import the sklearn module for GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create classifier\n",
    "clf=GaussianNB()\n",
    "#Start measuring training time\n",
    "t0=time()\n",
    "#Train the classifier\n",
    "clf.fit(features_train, labels_train)\n",
    "#Compute training time and print it\n",
    "print \"training time:\", round(time()-t0,3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('prediction time:', 0.216, 's')\n"
     ]
    }
   ],
   "source": [
    "#Start measuring predicton time                             \n",
    "t1=time()\n",
    "#Use the trained classifier to predict labels for the test features\n",
    "pred=clf.predict(features_test)\n",
    "#Compute and print prediction time\n",
    "print (\"prediction time:\", round(time()-t1,3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973265073948\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy of prediction                               \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(pred, labels_test)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Classifying our e-mails using Naive Bayes supervised classification algorithm gave us immediately a really good accuracy. It was really easy to implement and it was easy to run. It is worth to mention that Naive Bayes doesn't do well on expressions or anything comprised more than 1 word since Naive Bayes treats each word independently, it doesn't detect the relation - e.g. the hidden meaning - between words. Both the training and the predcition took only moments, so Naive Bayes looks like a good approach tackling this problem. Let's see how the other classifiers perform on the same dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 252.984 s\n"
     ]
    }
   ],
   "source": [
    "#Import sklearn module for SVM\n",
    "from sklearn import svm\n",
    "#Create classifier using first a linear kernel\n",
    "clf=svm.SVC(kernel='linear')\n",
    "#Start measuring training time\n",
    "t0=time()\n",
    "#Train classifier\n",
    "clf.fit(features_train, labels_train)\n",
    "#Compute training time and print it\n",
    "print \"training time:\", round(time()-t0,3), \"s\"#Start measuring predicton time   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction takes:  27.041 s\n"
     ]
    }
   ],
   "source": [
    "#Start measuring prediciton time\n",
    "t1=time()\n",
    "#Make predictions\n",
    "pred_SVM_linear_kernel=clf.predict(features_test)\n",
    "print \"Prediction takes: \", round(time()-t1, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984072810011\n"
     ]
    }
   ],
   "source": [
    "#Accuracy measurement\n",
    "accuracy_SVM_linear_kernel=accuracy_score(pred_SVM_linear_kernel, labels_test)\n",
    "print accuracy_SVM_linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Even though our SVM made a slightly better prediction on the dataset, it took awful much time to both train and predict. To speed training up I will use only 1 % of the training dataset. I expect some decrease in accuracy but on the other hand a sifnificant training time improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced training time:  0.157 s\n",
      "Prediction takes:  1.59 s\n",
      "0.884527872582\n"
     ]
    }
   ],
   "source": [
    "#Acquire 1 % of the training dataset \n",
    "features_train_reduced=features_train[:len(features_train)/100]\n",
    "labels_train_reduced=labels_train[:len(labels_train)/100]\n",
    "#The rest is the same as before\n",
    "t0=time()\n",
    "clf.fit(features_train_reduced, labels_train_reduced)\n",
    "print \"Reduced training time: \", round(time()-t0, 3), \"s\"\n",
    "t1=time()\n",
    "pred_SVM_linear_kernel=clf.predict(features_test)\n",
    "print \"Prediction takes: \", round(time()-t1, 3), \"s\"\n",
    "accuracy_SVM_linear_kernel=accuracy_score(pred_SVM_linear_kernel, labels_test)\n",
    "print accuracy_SVM_linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the reduced dataset training and prediction took about the same time than using Naive Bayes but we are nowhere close to the accuracy of ~97 %. Let's see what we get using a different kernel. Rbf is the default kernel being used if nothing is specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.178 s\n",
      "Prediction takes:  1.821 s\n",
      "0.616040955631\n"
     ]
    }
   ],
   "source": [
    "#Create classifier\n",
    "clf_rbf=svm.SVC(kernel='rbf')\n",
    "#Start measuring training time\n",
    "t0=time()\n",
    "#Train classifier\n",
    "clf_rbf.fit(features_train_reduced, labels_train_reduced)\n",
    "#Compute training time and print it\n",
    "print \"Training time:\", round(time()-t0,3), \"s\"\n",
    "#Start measuring prediciton time\n",
    "t1=time()\n",
    "#Make predictions\n",
    "pred_SVM_rbf_kernel=clf_rbf.predict(features_test)\n",
    "print \"Prediction takes: \", round(time()-t1, 3), \"s\"\n",
    "#Accuracy measurement\n",
    "accuracy_SVM_rbf_kernel=accuracy_score(pred_SVM_rbf_kernel, labels_test)\n",
    "print accuracy_SVM_rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Well, this is not a great accuracy, isn't it? Let's play around with the C (parameter of the error term). The decision boundary becomes more complex as C gets larger and larger (C=1, C=10, C=100, C=1000, C=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=10: 0.616040955631\n",
      "Accuracy with C=100: 0.616040955631\n",
      "Accuracy with C=1000: 0.821387940842\n",
      "Accuracy with C=10000: 0.892491467577\n"
     ]
    }
   ],
   "source": [
    "#C=10\n",
    "clf_rbf_C10=svm.SVC(C=10, kernel='rbf')\n",
    "clf_rbf_C10.fit(features_train_reduced, labels_train_reduced)\n",
    "pred_SVM_rbf_C10_kernel=clf_rbf_C10.predict(features_test)\n",
    "accuracy_SVM_rbf_C10_kernel=accuracy_score(pred_SVM_rbf_C10_kernel, labels_test)\n",
    "print \"Accuracy with C=10:\", accuracy_SVM_rbf_C10_kernel \n",
    "#C=100\n",
    "clf_rbf_C100=svm.SVC(C=100, kernel='rbf')\n",
    "clf_rbf_C100.fit(features_train_reduced, labels_train_reduced)\n",
    "pred_SVM_rbf_C100_kernel=clf_rbf_C100.predict(features_test)\n",
    "accuracy_SVM_rbf_C100_kernel=accuracy_score(pred_SVM_rbf_C100_kernel, labels_test)\n",
    "print \"Accuracy with C=100:\", accuracy_SVM_rbf_C100_kernel \n",
    "#C=1000\n",
    "clf_rbf_C1000=svm.SVC(C=1000, kernel='rbf')\n",
    "clf_rbf_C1000.fit(features_train_reduced, labels_train_reduced)\n",
    "pred_SVM_rbf_C1000_kernel=clf_rbf_C1000.predict(features_test)\n",
    "accuracy_SVM_rbf_C1000_kernel=accuracy_score(pred_SVM_rbf_C1000_kernel, labels_test)\n",
    "print \"Accuracy with C=1000:\", accuracy_SVM_rbf_C1000_kernel \n",
    "#C=10000\n",
    "clf_rbf_C10000=svm.SVC(C=10000, kernel='rbf')\n",
    "clf_rbf_C10000.fit(features_train_reduced, labels_train_reduced)\n",
    "pred_SVM_rbf_C10000_kernel=clf_rbf_C10000.predict(features_test)\n",
    "accuracy_SVM_rbf_C10000_kernel=accuracy_score(pred_SVM_rbf_C10000_kernel, labels_test)\n",
    "print \"Accuracy with C=10000:\", accuracy_SVM_rbf_C10000_kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's get back to the full training dataset. It should significantly improve the prediction accuracy, now that C value is optimized, let's see how much does the full training dataset improves our predctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 175.746 s\n",
      "Prediction takes:  17.512 s\n",
      "0.990898748578\n"
     ]
    }
   ],
   "source": [
    "#Create classifier\n",
    "clf_rbf=svm.SVC(C=10000,kernel='rbf')\n",
    "#Start measuring training time\n",
    "t0=time()\n",
    "#Train classifier\n",
    "clf_rbf.fit(features_train, labels_train)\n",
    "#Compute training time and print it\n",
    "print \"Training time:\", round(time()-t0,3), \"s\"\n",
    "#Start measuring prediciton time\n",
    "t1=time()\n",
    "#Make predictions\n",
    "pred_SVM_rbf_kernel=clf_rbf.predict(features_test)\n",
    "print \"Prediction takes: \", round(time()-t1, 3), \"s\"\n",
    "#Accuracy measurement\n",
    "accuracy_SVM_rbf_kernel=accuracy_score(pred_SVM_rbf_kernel, labels_test)\n",
    "print accuracy_SVM_rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We reached more than 99 % accuracy, however the training and prediction time increased significantly compared to using Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's tackle the problem using Decision Tree algorithm this time. As we have ~3800 features in our dataset, let's set the min_sample_split parameters to 40. Setting this parameter to an even smaller value would most likely result to overfitting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 57.775 s\n",
      "Prediction takes:  0.028 s\n",
      "0.978953356086\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf=tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "#Start measuring training time\n",
    "t0=time()\n",
    "clf=clf.fit(features_train, labels_train)\n",
    "#Compute training time and print it\n",
    "print \"Training time:\", round(time()-t0,3), \"s\"\n",
    "#Start measuring prediciton time\n",
    "t1=time()\n",
    "#Make predictions\n",
    "predict=clf.predict(features_test)\n",
    "print \"Prediction takes: \", round(time()-t1, 3), \"s\"\n",
    "#Accuracy measurement\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(predict, labels_test)\n",
    "print acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We explored the power of tuning the algorithm parameters in terms of training and prediciton time as well as accuracy. Another way to control the complexity of an algorithm is via the number of features that we use in training/testing. The more features the algorithm has available, the more potential there is for a complex fit. Let's reduce the number of features all the way down to 10 % of the original value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess(words_file = \"word_data.pkl\", \n",
    "               authors_file=\"email_authors.pkl\"):\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = cPickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = sklearn.model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "    ### setting percentile parameter to 1 results having 379 features instead of 3785\n",
    "    selector = SelectPercentile(f_classif, percentile=1)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "Training time: 5.14 s\n",
      "Prediction takes:  0.005 s\n",
      "0.966439135381\n",
      "379\n"
     ]
    }
   ],
   "source": [
    "#Training and testing time, accuracy in case of 10 % of the original features using the same parameters as before\n",
    "\n",
    "from sklearn import tree\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "clf=tree.DecisionTreeClassifier(min_samples_split=40)\n",
    "#Start measuring training time\n",
    "t0=time()\n",
    "clf=clf.fit(features_train, labels_train)\n",
    "#Compute training time and print it\n",
    "print \"Training time:\", round(time()-t0,3), \"s\"\n",
    "#Start measuring prediciton time\n",
    "t1=time()\n",
    "#Make predictions\n",
    "predict=clf.predict(features_test)\n",
    "print \"Prediction takes: \", round(time()-t1, 3), \"s\"\n",
    "#Accuracy measurement\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(predict, labels_test)\n",
    "print acc\n",
    "print len(features_train[0])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
